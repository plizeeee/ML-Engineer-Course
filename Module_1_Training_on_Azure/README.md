# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement:**
This dataset contains data about we customers in a marketing campaign for a portugese bank collected by UCI Irvine. We seek to predict whether or not a client will subscribe to a term deposit.

**In 1-2 sentences, explain the solution: e.g.**
We used machine learning on the dataset to predict whether or not clients subscribed to a term of deposit. We used two approaches to train the models, one with hyperdrive, and one with automl in Azure. The best performing model was the automl solution, creating a voting ensemble of 6 different types of models.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The Scikit-learn pipeline included data retreived from the UCI dataset in a tabular dataset factory object. A logistic regression model was trained and we tuned 2 hyperparameters (the regularization and the number of iterations) using hyperdrive.

**What are the benefits of the parameter sampler you chose?**
I chose ranom search since it is much faster than grid search and finds solutions that tend to be close to optimal through randomely sampling a grid.


**What are the benefits of the early stopping policy you chose?**
The early stopping I used was "bandit" and the benefit of using this early stopping is that training will stop if models stop improving less than a slack factor (to ensure to don't get stuck with very long training runs or overfit the model too much)

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
I used the following parameters for autoML 
```
automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='AUC_weighted',
    training_data=df_all,
    label_column_name='y',
    n_cross_validations=5)
```
- experiment_timeout_minutes sets the time it takes for the experiment to automatically stop, which for this experiment must be 30 minutes or it would timout too early. If I set it to longer than 30 minutes then it would cut into the time it takes to complete the lab, so this was the best value to use.
- Classification was used as the task, since the goal is to predict a binary outcome ("yes" or "no" if clients will subsribe to term deposits)
- I used AUC_weighted as the primary metric, since the classes are substantially imbalaned (the minority class is less than 20% of examples in this case) so this metric would help improve accuracy on the minority class.
- the training data is a dataframe of the marketting campaign data - includes both features and the outcome variable
- label_column_name is the name of the outcome variable (in this case it's called "y")
- n_cross_validation is the number of folds for cross validation when validating the model - this ensures that the model isn't overfitting but increasing the time it takes to train the model since it will be trained by folding the data into 5.

The Model generated by automl was a voting ensemble. There was a combination of 6 different models (including XGboost, lightgbm, and other models that perform well on small datasets). There are too many hyperparameters to discuss in much detail but it seems lightgbm got the highest weight in the voting ensemble (0.467) whereas the rest of the weight is fairly evenly split between the rest of the 6 models, so it seems LGBM contributes most to the model.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The best performing model was the ensemble generated by automl - The performance difference was not large in terms of accurcy (likely due to the imbalanced nature of the dataset), where the Scikit-learn Pipeline achieved an accuracy of 91.3% and autoML an accuracy of 91.4%. The autoML had a pretty good AUC score at 0.95, but there wasn't one for the Scikit-learn Pipeline so it is difficult to compare this better metric. The Scikit-learn Pipeline only trained a logistic regression model, whereas automl trained a plethora of model types with various combinations of hyperparameters. It was clear that the automl model was significantly easier to initialize and train (it also easily trained a bunch of different models, whereas for hyperdrive we only tuned a LR model, so tuning a ton of different models would have been difficult).

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Some areas of improvement include using better measures for imbalanced datasets (a head-to-head comparison using AUC would be much better). Also could be good to generate synthetic data for the minority class to get more examples. Another way to improve performance could be collecting extra types of data (i.e. repayment history of clients could be a good predictor for if clients subscribe to a term deposit).
